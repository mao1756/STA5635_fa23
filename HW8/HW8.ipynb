{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8\n",
    "\n",
    "Author: Mao Nishino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from IPython.display import display_markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "Download the dataset hmm_pb1.csv from Canvas. It represents a sequence of\n",
    "dice rolls x from the Dishonest casino model discussed in class. The model parameters\n",
    "are exactly those presented in class. The states of Y are 1=’Fair’ and 2=’Loaded’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the sequence of rolls. \n",
    "# 1D array where each element represents the result of each dice roll.\n",
    "pb1 = np.array(pd.read_csv('./hmm_pb1.csv', header=None)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HMM\n",
    "\"\"\"\n",
    "Data Structures\n",
    "    pi(np.ndarray): The initial probabilities. 1D array of size 2.\n",
    "                    pi[0] = P(the first dice is fair)\n",
    "                    pi[1] = P(the first dice is loaded)\n",
    "\n",
    "    a(np.ndarray): The transition probabilities. 2d array of size (2,2).\n",
    "                    a[i,j] = P(Y_{n+1}=j+1|Y_n = i+1)\n",
    "\n",
    "    b(np.ndarray): The emission probabilities. 2d array of size (2,6).\n",
    "                   b[i,j] = P(X_n=j+1|Y_n=i+1)\n",
    "    Here, Y_n = the hidden state. Y_n=1=>fair, Y_n=2=>loaded.\n",
    "    X_n = the observed dice roll at time n.    \n",
    "\"\"\"\n",
    "pi = np.array([0.5, 0.5])\n",
    "a = np.array(\n",
    "    [[0.95, 0.05],\n",
    "     [0.05, 0.95]]\n",
    ")\n",
    "b = np.array(\n",
    "    [[1./6, 1./6, 1./6, 1./6, 1./6, 1./6],\n",
    "    [1./10, 1./10, 1./10, 1./10, 1./10, 1./2]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem (a)\n",
    "Implement the Viterbi algorithm and find the most likely sequence y that gener-\n",
    "ated the observed x. Use the log probabilities, as shown in the HMM slides from\n",
    "Canvas. Report the obtained sequence y of 1’s and 2’s for verification. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The following shows the most probable sequence of the hidden state."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data Structures Specific To This Problem\n",
    "    c (np.ndarray): the log Viterbi probability. \n",
    "        It has 2 rows and pb1.shape[0] columns.\n",
    "        The kth row represents the log-probabilities at each time for\n",
    "        the hidden state k+1. (k=0=>Fair, k=1=>loaded)\n",
    "        The jth column represents the log-probabilities for each hidden state\n",
    "        at time j.\n",
    "\n",
    "    ptr (np.ndarray): the hidden states that maximizes\n",
    "                       the Viterbi probability at each time.\n",
    "                       The format is the same as -C.\n",
    "                       Ptr in the page 27 of HMM slides.\n",
    "\n",
    "    y_star (np.ndarray): the most likely sequence of the hidden state.\n",
    "\"\"\"\n",
    "\n",
    "c = np.zeros((2, pb1.shape[0]))\n",
    "ptr = np.zeros((2, pb1.shape[0]))\n",
    "y_star = -1*np.ones_like(pb1) # Put -1 to make debugging easier\n",
    "\n",
    "\"\"\"\n",
    "The Viterbi Algorithm\n",
    "\"\"\"\n",
    "\n",
    "# Initialization\n",
    "c[:,0] = np.log(b[:, pb1[0]-1]*pi)\n",
    "\n",
    "# Loops\n",
    "for t in range(1, pb1.shape[0]):\n",
    "    # Reshape so that c[:,t-1] would be added to each column of log(a) \n",
    "    # by broadcasting\n",
    "    logaC = np.log(a) + c[:, t-1].reshape(-1,1)\n",
    "    c[:, t] = np.log(b[:, pb1[t]-1]) + np.max(logaC, axis = 0)\n",
    "    ptr[:, t] = np.argmax(logaC, axis = 0)\n",
    "\n",
    "# We now find y_star.\n",
    "# Initialization\n",
    "y_star[-1] = np.argmax(c[:, -1])\n",
    "\n",
    "# Start from the last index pb1.shape[0]-1\n",
    "# last -1 means we go backward\n",
    "for t in range(pb1.shape[0]-1, 0, -1):\n",
    "    y_star[t-1] = ptr[y_star[t], t]\n",
    "\n",
    "display_markdown(\"The following shows the most probable sequence of the hidden state.\", raw = True)\n",
    "print(y_star+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question (b)\n",
    "Implement the forward and backward algorithms and run them on the observed\n",
    "x. You should memorize a common factor ut for the αkt to avoid floating point\n",
    "underflow, since αkt quickly become very small. The same holds for $β_{t}^{k}$ . Report\n",
    "$\\alpha_{135}^1/\\alpha_{135}^2/$ and $\\beta_{135}^1/\\beta_{135}^2$, where the counting starts from t = 0. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The following shows $\\alpha_{135}^1/\\alpha_{135}^2$."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.6344086021505375, 5.5035735614248, 8.748795642368156]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The following shows $\\beta_{135}^1/ \\beta_{135}^2$."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.201606143659003, 2.2675804409135245, 0.9492586264514562]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data structures specific to this problem\n",
    "\n",
    "alpha (np.ndarray) : the forward probability of the shape 2 x pb.shape[0]\n",
    "        The kth row represents the forward-probabilities at each time for\n",
    "        the hidden state k+1. (k=0=>Fair, k=1=>loaded)\n",
    "        The jth column represents the backward probabilities for each hidden state\n",
    "        at time j.\n",
    "beta (np.ndarray): the backward probability of the shape 2 x pb.shape[0]\n",
    "\"\"\"\n",
    "\n",
    "def forward_backward(pb: np.ndarray, pi, a, b):\n",
    "    \"\"\" The Forward Backward Algorithm\n",
    "\n",
    "    Args:\n",
    "    pb: the dataset\n",
    "    pi: initial prob\n",
    "    a: transition prob\n",
    "    b: emission prob\n",
    "    \n",
    "    Returns:\n",
    "    alpha: the forward probability\n",
    "    beta: the backward probability\n",
    "    \"\"\"\n",
    "\n",
    "    alpha = np.zeros((2, pb.shape[0]))\n",
    "    beta = np.zeros((2, pb.shape[0]))\n",
    "\n",
    "    # Find the foward probability\n",
    "    alpha[:, 0] = b[:, pb[0]-1]*pi\n",
    "\n",
    "    for t in range(1, pb.shape[0]):\n",
    "        alpha_numerator = b[:, pb[t]-1] * np.sum(a*(alpha[:,t-1].reshape(-1,1)), axis = 0)\n",
    "        # Normalize to avoid underflow\n",
    "        alpha_denominator = np.sum(alpha_numerator)\n",
    "        alpha[:, t] = alpha_numerator/alpha_denominator\n",
    "\n",
    "    # Find the backward probability\n",
    "    beta[:, -1]  = 1\n",
    "\n",
    "    for t in range(pb.shape[0]-2, -1, -1):\n",
    "        beta_numerator = np.sum(a*(beta[:,t+1]*b[:, pb[t+1]-1]), axis = 1)\n",
    "        # Normalize to avoid underflow\n",
    "        beta_denominator =  np.sum(beta_numerator)\n",
    "        beta[:, t] = beta_numerator/beta_denominator\n",
    "\n",
    "    return alpha, beta\n",
    "\n",
    "# Display the result\n",
    "alpha, beta = forward_backward(pb1, pi, a, b)\n",
    "display_markdown(\"The following shows $\\\\alpha_{135}^1/\\\\alpha_{135}^2$.\", raw=True)\n",
    "print([alpha[0][t]/alpha[1][t] for t in [1,3,5]])\n",
    "display_markdown(\"The following shows $\\\\beta_{135}^1/ \\\\beta_{135}^2$.\", raw=True)\n",
    "print([beta[0][t]/beta[1][t] for t in [1,3,5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "Download the dataset hmm_pb2.csv from Canvas. It represents a sequence of\n",
    "10000 dice rolls x from the Dishonest casino model but with other values for the a and\n",
    "b parameters than those from class. Having so many observations, you are going to\n",
    "learn the model parameters.\n",
    "Implement and run the Baum-Welch algorithm using the forward and backward\n",
    "algorithms that you already implemented for Pb 1. You can initialize the π,a,b with\n",
    "your guess, or with some random probabilities (make sure that π sums to 1 and that\n",
    "aij ,bik sum to 1 for each i). The algorithm converges quite slowly, so you might need\n",
    "to run it for up 1000 iterations or more for the parameters to converge.\n",
    "Report the values of π,a,b that you have obtained. (4 points)\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the sequence of rolls. \n",
    "# 1D array where each element represents the result of each dice roll.\n",
    "pb2 = np.array(pd.read_csv('./hmm_pb2.csv', header=None)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The following shows the learned initial probability."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.90171704e-294 1.00000000e+000]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The following shows the learned transition probability."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.70708794 0.29291206]\n",
      " [0.01238918 0.98761082]]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The following shows the learned emission probability."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09569917 0.11414408 0.07285264 0.04437644 0.57558343 0.09734424]\n",
      " [0.20097134 0.20571544 0.19349497 0.20178681 0.10500669 0.09302474]]\n"
     ]
    }
   ],
   "source": [
    "def baum_welch(pb, pi, a, b, niter = 10):\n",
    "    \"\"\" Baum-Welch Algorithm\n",
    "    \n",
    "    Given the dataset, it learns a HMM using the Baum-Welch algorithm.\n",
    "\n",
    "    Args:\n",
    "        pb: the dataset.\n",
    "        pi: the initial guess on the initial probablity.\n",
    "        a: the initial guess on the transition probablity.\n",
    "        b: the initial guess on the emission probablity.\n",
    "    Returns:\n",
    "        _pi: the initial probabilty.\n",
    "        _a: the transition probability.\n",
    "        _b: the emission probability.\n",
    "        Formats are the same as previously defined.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialization\n",
    "    _pi = pi\n",
    "    _a = a\n",
    "    _b = b\n",
    "\n",
    "    # Create delta(x_t = k).\n",
    "    # It should have the shape 6*pb.shape[0].\n",
    "    encoder = OneHotEncoder(categories = [[1,2,3,4,5,6]], sparse_output=False)\n",
    "    encoded_pb = np.transpose(encoder.fit_transform(pb.reshape(-1,1)))\n",
    "    \n",
    "    for i in range(niter):\n",
    "        alpha, beta = forward_backward(pb, _pi, _a, _b)\n",
    "        # E-Step\n",
    "\n",
    "        # Calculation of xi. xi should have the size 2 x 2 x pb.shape[0],\n",
    "        # where the first index represents i, the second j, the last t\n",
    "        # where i,j,t are indexes from the slides.\n",
    "        \n",
    "        # Calculate b_x(t+1)^j\n",
    "        bx = _b[:, np.roll(pb-1,-1)]\n",
    "\n",
    "        xi_numerator = alpha.reshape(2, 1, -1) * \\\n",
    "            _a.reshape(2, 2, 1) * \\\n",
    "            np.roll(beta.reshape(1, 2, -1), shift = -1, axis = -1) * \\\n",
    "            bx.reshape(1, 2, -1)\n",
    "        \n",
    "        xi_denominator = np.sum(xi_numerator, axis=(0,1)).reshape(1,1,-1)\n",
    "        xi = xi_numerator/xi_denominator\n",
    "        assert xi_denominator.min() != 0\n",
    "\n",
    "        # Calculation of gamma. gamma should have size 2 x pb.shape[0].\n",
    "        gamma_numerator = alpha*beta\n",
    "        gamma_denominator = np.sum(gamma_numerator, axis = 0).reshape(1,-1)\n",
    "        gamma = gamma_numerator/gamma_denominator\n",
    "        assert gamma_denominator.min() != 0\n",
    "\n",
    "        # M-step\n",
    "        _pi = gamma[:, 0]\n",
    "        _a = np.sum(xi[:,:,:-1], axis = 2)/np.sum(gamma[:, :-1], axis = 1).reshape(-1,1)\n",
    "        _b = np.sum(gamma.reshape(2,1,-1)*encoded_pb.reshape(1,6,-1), axis = 2)/ \\\n",
    "            np.sum(gamma, axis = 1).reshape(-1,1)\n",
    "    \n",
    "    return _pi, _a, _b\n",
    "\n",
    "pi = np.array([0.5, 0.5])\n",
    "a = np.array(\n",
    "    [[0.95, 0.05],\n",
    "     [0.05, 0.95]]\n",
    ")\n",
    "b = np.array(\n",
    "    [[1./6, 1./6, 1./6, 1./6, 1./6, 1./6],\n",
    "    [1./10, 1./10, 1./10, 1./10, 1./10, 1./2]]\n",
    ")\n",
    "\n",
    "new_pi, new_a, new_b = baum_welch(pb2, pi, a, b, 1000)\n",
    "display_markdown(\"The following shows the learned initial probability.\", raw=True)\n",
    "print(new_pi)\n",
    "display_markdown(\"The following shows the learned transition probability.\", raw=True)\n",
    "print(new_a)\n",
    "display_markdown(\"The following shows the learned emission probability.\", raw=True)\n",
    "print(new_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
