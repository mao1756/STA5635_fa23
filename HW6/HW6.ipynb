{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import Relevant Libraries\n","import os\n","import numpy as np\n","import dask.dataframe as dd\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import accuracy_score, roc_curve\n","from scipy.special import expit\n","import matplotlib.pyplot as plt\n","from IPython.display import display, Markdown"]},{"cell_type":"markdown","metadata":{},"source":["# Homework 6\n","Author: Mao Nishino\n","\n","## Problem 1\n","Implement Logitboost using univariate (based on a single feature, with intercept) lin-\n","ear regressors as weak learners. At each boosting iteration choose the weak learner that\n","obtains the largest reduction in the loss function on the training set D = {(xi, yi), i =\n","1, ..., N }, with yi ∈{0, 1},\n","$\\begin{equation} L = \\sum_{i=1}^{N}\\ln(1+\\log(-\\tilde{y}_ih(\\bf{x}_i))) \\tag*{} \\end{equation}$ \n","\n","where  ̃yi = 2yi −1 take values ±1 and h(x) = h1(x) + ... + hk(x) is the boosted\n","classifier. Please note that the Logitboost algorithm from the slides uses yi ∈ {0, 1}\n","and the loss uses  ̃yi ∈{−1, 1}."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'np' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/home/mao1756/STA5635_fa23/HW6/HW6.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mao1756/STA5635_fa23/HW6/HW6.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mao1756/STA5635_fa23/HW6/HW6.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mlog(\u001b[39m1\u001b[39m\u001b[39m+\u001b[39mnp\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mx))\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mao1756/STA5635_fa23/HW6/HW6.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m v_log_one_exp \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvectorize(log_one_exp)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mao1756/STA5635_fa23/HW6/HW6.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mAddBias\u001b[39;00m():\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mao1756/STA5635_fa23/HW6/HW6.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Prepends columns of 1s to the dataset\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/mao1756/STA5635_fa23/HW6/HW6.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"]}],"source":["NITER = 100\n","\n","def log_one_exp(x):\n","    \"\"\" Aux function for the calculation of logloss\n","    Copied from my solution in HW3\n","    \"\"\"\n","    if x < 0:\n","        return np.log(1+np.exp(x))\n","    else:\n","        return np.log(1+np.exp(-x))\n","\n","v_log_one_exp = np.vectorize(log_one_exp)\n","\n","class AddBias():\n","    \"\"\" Prepends columns of 1s to the dataset\n","    \"\"\"\n","    def fit(self, X, y = None):\n","        return self\n","\n","    def transform(self, X):\n","        return np.hstack([np.ones((X.shape[0], 1)), X])\n","\n","class LogitBoostClassifier():\n","    \"\"\" Binary Classification using LogitBoost\n","    Each weak learner h_k(x)=ax_i+b is characterized by the tuple (i, [a,b])\n","    where i is the feature to use and a, b are coefficients. \n","    The strong learner is characterized by the list of (i, [a,b]).\n","    \n","    Attributes:\n","        k (int): the number of total iterations\n","        strong_learner (list) : the strong learner described above.\n","        train_errors (list): the training loss vs the iteration number\n","    \"\"\"\n","    def __init__(self, k: int):\n","        self.k = k\n","        self.strong_learner = []\n","        self.train_errors = []\n","\n","    def _eval_learner(self, strong_learner, X) -> float:\n","        \"\"\" Using a strong learner h,\n","            output an array of h(x)'s\n","            where x is each row of X\n","\n","            Args:\n","                strong_learner: a strong learner described above\n","                X(np.array): the matrix of data\n","            Returns:\n","                h(X) (np.array): array of h(x) where x is each row of X\n","        \"\"\"\n","        pred = 0 # If stronglearner = [], return 0\n","        for weak_learner in strong_learner:\n","            a = weak_learner[1][0]\n","            b = weak_learner[1][1]\n","            i = weak_learner[0]\n","            pred += a*X[:,i]+b\n","        return pred\n","\n","    def fit(self, X, y):\n","        for i in self.k:\n","            p = expit(self._eval_learner(self.strong_learner, X))\n","            w = p*(1-p)\n","            z = (y-p)/w\n","\n","        return self\n","    \n","    def predict(self, X):\n","        pass\n","    \n","    def predict_proba(self, X):\n","        pass\n","\n","class HW6():\n","    \"\"\" A class that achieves everything required for this assignment.\n","    Attributes:\n","    trainloss_300 (numpy.array) : an array that returns the training loss\n","                                for the i+1th iteration with 300 features\n","    misclass_all (numpy.array) : first column contains k, \n","    second contains train errors, the third contains test errors\n","    train30_pred_proba_y (numpy.array) : an array that contains the predicted value for\n","                                    the case with 30 features for ROC\n","    test30_pred_proba_y (numpy.array) : an array that contains the predicted value for\n","                                    the case with 30 features for ROC\n","    All of the arrays are of the format\n","    [k=500, k=300, k=100, k=30, k=10]\n","    \"\"\"\n","\n","    def __init__(self, \n","                 train_x : dd.DataFrame,\n","                 train_y : dd.DataFrame,\n","                 test_x : dd.DataFrame,\n","                 test_y : dd.DataFrame) -> None:\n","        \"\"\" Finds all of the attributes.\n","            Args:\n","            train_x, train_y : training data\n","            test_x, test_y : test data\n","        \"\"\"\n","        ks = [500, 300, 100, 30, 10]\n","        train_errors = []\n","        test_errors = []\n","        for k in ks:\n","            pipe = Pipeline([('scaler', StandardScaler()),\n","                            ('addbias', AddBias()),\n","                            ('lb', LogitBoostClassifier())])\n","            pipe.fit(train_x, train_y)\n","            # Calculate train errors\n","            train_pred_y = pipe.predict(train_x)\n","            train_error = 1-accuracy_score(train_y, train_pred_y)\n","            train_errors.append(train_error)\n","            # Calculate test errors\n","            test_pred_y = pipe.predict(test_x)\n","            test_error = 1-accuracy_score(test_y, test_pred_y)\n","            test_errors.append(test_error)\n","            \n","            if k == 300:\n","                self.trainloss_300 = pipe['lb'].train_errors\n","            if k == 30:\n","                self.train30_pred_proba_y = pipe.predict_proba(train_x)\n","                self.test30_pred_proba_y = pipe.predict_proba(test_x)\n","\n","        self.misclass_all = [list(x) for x in zip(ks, train_errors, test_errors)]\n","    \n","    def plot_trainloss_300(self):\n","        plt.figure(figsize = (10,6))\n","        plt.plot(range(1,300+1), self.trainloss_30)\n","        plt.title('# Iteration vs Training Loss When #Features = 300')\n","        plt.xlabel('# Iteration')\n","        plt.ylabel('Training_Loss')\n","        plt.grid(True)\n","        plt.show()\n","    \n","    def plot_misclass_all(self):\n","        plt.figure(figsize = (10,6))\n","        ks  = [item[1] for item in self.misclass_all]\n","        train_errors = [item[2] for item in self.misclass_all]\n","        test_errors  = [item[3] for item in self.misclass_all] \n","        plt.plot(ks, train_errors, marker = 'o', label = 'Training Error')\n","        plt.plot(ks, test_errors, marker = 'x', label = 'Test Error')        \n","        plt.title('k vs Misclassification Errors')\n","        plt.xlabel('k')\n","        plt.ylabel('Misclassification Errors')\n","        plt.legend()\n","        plt.grid(True)\n","        plt.show()\n","    \n","    def report_table(self):\n","        #desired_features  = [item[0] for item in self.misclass_all]\n","        num_nonzero  = [item[1] for item in self.misclass_all]\n","        train_errors = [item[2] for item in self.misclass_all]\n","        test_errors  = [item[3] for item in self.misclass_all]\n","        table = {\n","         #   '#Desired_Features' : desired_features,\n","            '#Features' : num_nonzero,\n","            'Training Errors' : train_errors,\n","            'Test Errors' : test_errors\n","        }\n","        df = pd.DataFrame(table)\n","        display(df)\n","    \n","    def show_roc(self, y, test_y):\n","        fpr_train, tpr_train, thr = \\\n","            roc_curve(y, self.train100_pred_proba_y)\n","        fpr_test, tpr_test, thr =  \\\n","            roc_curve(test_y, self.test100_pred_proba_y)\n","\n","        # Plot ROC\n","        plt.figure(figsize=(10, 6))\n","        plt.plot(fpr_train, tpr_train, marker = 'o', label = 'Training ROC')\n","        plt.plot(fpr_test, tpr_test, marker = 'x', label = 'Test ROC')\n","        plt.xlabel('False Positive Rate (FPR)')\n","        plt.ylabel('True Positive Rate (TPR)')\n","        plt.legend()\n","        plt.title('ROC curves for k = 30')\n","        plt.grid(True)\n","        plt.show()\n","  "]},{"cell_type":"markdown","metadata":{},"source":["### Question (a)\n","Using the Gisette data, train a Logitboost classifier on the training set, with\n","k ∈{10, 30, 100, 300, 500} boosting iterations. Plot the training loss vs iteration\n","number for k = 300. Report in a table the misclassification errors on the training\n","and test set for the models obtained for all these k. Plot the misclassification\n","errors on the training and test set vs k. Also plot the train and test ROC curves\n","of the obtained model with 30 features."]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":2}
